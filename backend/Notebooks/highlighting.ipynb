{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import enchant\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/gutenberg_dictionary.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m../app\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtext_highlighter\u001b[39;00m \u001b[39mimport\u001b[39;00m highlight_non_french_words\n",
      "File \u001b[0;32m~/Documents/backend-learning-wikinarrator/backend/Notebooks/../app/text_highlighter.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='file:///~/Documents/backend-learning-wikinarrator/backend/Notebooks/../app/text_highlighter.py?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39munidecode\u001b[39;00m\n\u001b[1;32m      <a href='file:///~/Documents/backend-learning-wikinarrator/backend/Notebooks/../app/text_highlighter.py?line=3'>4</a>\u001b[0m \u001b[39m# 336531 French words. From https://github.com/chrplr/openlexicon/blob/master/datasets-info/Liste-de-mots-francais-Gutenberg,\u001b[39;00m\n\u001b[1;32m      <a href='file:///~/Documents/backend-learning-wikinarrator/backend/Notebooks/../app/text_highlighter.py?line=4'>5</a>\u001b[0m \u001b[39m# to which I added some words like 'puisqu'', 'aujourd'hui, quantique' etc.\u001b[39;00m\n\u001b[0;32m----> <a href='file:///~/Documents/backend-learning-wikinarrator/backend/Notebooks/../app/text_highlighter.py?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m./data/gutenberg_dictionary.txt\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='file:///~/Documents/backend-learning-wikinarrator/backend/Notebooks/../app/text_highlighter.py?line=6'>7</a>\u001b[0m     words_list \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39msplitlines()\n\u001b[1;32m      <a href='file:///~/Documents/backend-learning-wikinarrator/backend/Notebooks/../app/text_highlighter.py?line=7'>8</a>\u001b[0m french_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(words_list)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/gutenberg_dictionary.txt'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../app\")\n",
    "from text_highlighter import highlight_non_french_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 336531 French words. From https://github.com/chrplr/openlexicon/blob/master/datasets-info/Liste-de-mots-francais-Gutenberg,\n",
    "# to which I added some words like 'puisqu'', 'aujourd'hui, quantique' etc.\n",
    "with open('../data/gutenberg_dictionary.txt') as f:\n",
    "    mots_list = f.read().splitlines()\n",
    "mots = set(mots_list)\n",
    "\n",
    "text = open(\"einstein.txt\", 'r', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.5 ms ± 70.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "t = highlight_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_french_words(text: str):\n",
    "    invalid_words = []\n",
    "\n",
    "    french_words_regex = r\"[A-Za-zÀ-ÖØ-öø-ÿœ]{3,}\"\n",
    "    # french_words_regex = r\"[a-zA-ZÀ-ÿ]{3,}\"\n",
    "    # french_words_regex = r\"[a-zA-ZÀ-ÿ-]{2,}\"\n",
    "\n",
    "    text_words = re.findall(french_words_regex, text)\n",
    "    for word in text_words:\n",
    "\n",
    "        # Check if lowercased and ascii'd word is in the dictionary (e.g. 'Sœur' to 'soeur')\n",
    "        if word.lower() not in mots and unidecode.unidecode(word.lower()) not in mots:\n",
    "            invalid_words.append(word)\n",
    "    \n",
    "    # Remove noms propres, but keep acronyms ?\n",
    "    invalid_words = [word for word in invalid_words if (not word[0].isupper() or word.isupper())]\n",
    "    \n",
    "    return invalid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enchant.checker import SpellChecker\n",
    "\n",
    "def find_non_french_words_2(text: str):\n",
    "    chkr = SpellChecker(\"fr_FR\")\n",
    "    chkr.set_text(text)\n",
    "    invalid_words = [err.word for err in chkr]\n",
    "    return invalid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "invalid_words = find_non_french_words(text)\n",
    "# invalid_words = find_non_french_words_2(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alb',\n",
       " 'helvético',\n",
       " 'insurge',\n",
       " 'EPFZ',\n",
       " 'EPFZ',\n",
       " 'annus',\n",
       " 'brownien',\n",
       " 'brownien',\n",
       " 'neue',\n",
       " 'honoris',\n",
       " 'austro',\n",
       " 'hyperconnexion',\n",
       " 'annus',\n",
       " 'brownien',\n",
       " 'newtonienne',\n",
       " 'électrodynamique',\n",
       " 'tenseurs',\n",
       " 'würfelt',\n",
       " 'nicht',\n",
       " 'EPR',\n",
       " 'instrumentalisée',\n",
       " 'cosigne',\n",
       " 'maccarthysme',\n",
       " 'maccarthysme',\n",
       " 'maccarthysme',\n",
       " 'FBI',\n",
       " 'URSS',\n",
       " 'anationale',\n",
       " 'SAT',\n",
       " 'socio',\n",
       " 'franquisme',\n",
       " 'FBI',\n",
       " 'FBI',\n",
       " 'NAACP',\n",
       " 'coccygodynie',\n",
       " 'eBay',\n",
       " 'life',\n",
       " 'uvres',\n",
       " 'berlinoise',\n",
       " 'illuminatrice',\n",
       " 'astrologischer',\n",
       " 'képlérienne',\n",
       " 'téléologiquement',\n",
       " 'caloporteur',\n",
       " 'doing',\n",
       " 'the',\n",
       " 'same',\n",
       " 'thing',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " 'and',\n",
       " 'expecting',\n",
       " 'different',\n",
       " 'results',\n",
       " 'bewegter',\n",
       " 'électrodynamique',\n",
       " 'einen',\n",
       " 'die',\n",
       " 'und',\n",
       " 'betreffenden',\n",
       " 'heuristischen',\n",
       " 'trad',\n",
       " 'die',\n",
       " 'eines',\n",
       " 'von',\n",
       " 'seinem',\n",
       " 'abhängig',\n",
       " 'trad',\n",
       " 'trad',\n",
       " 'gravitationnelles',\n",
       " 'trad',\n",
       " 'uvres',\n",
       " 'CNRS',\n",
       " 'trad',\n",
       " 'coll',\n",
       " 'ISBN',\n",
       " 'FRBNF',\n",
       " 'trad',\n",
       " 'coll',\n",
       " 'ISBN',\n",
       " 'OCLC',\n",
       " 'FRBNF',\n",
       " 'coll',\n",
       " 'ISBN',\n",
       " 'politico',\n",
       " 'trad',\n",
       " 'coll',\n",
       " 'ISBN',\n",
       " 'OCLC',\n",
       " 'mole',\n",
       " 'astrocytes',\n",
       " 'gliales',\n",
       " 'gliales',\n",
       " 'dudit',\n",
       " 'coll',\n",
       " 'ISBN',\n",
       " 'coll',\n",
       " 'ISBN',\n",
       " 'ISBN',\n",
       " 'ISBN',\n",
       " 'coll',\n",
       " 'ISBN',\n",
       " 'coll',\n",
       " 'ISBN',\n",
       " 'eines',\n",
       " 'ISBN',\n",
       " 'trad',\n",
       " 'and',\n",
       " 'racism',\n",
       " 'coll',\n",
       " 'ISBN',\n",
       " 'OCLC',\n",
       " 'FRBNF',\n",
       " 'coll',\n",
       " 'RBA',\n",
       " 'coll',\n",
       " 'PUF',\n",
       " 'ISBN',\n",
       " 'IHÉS',\n",
       " 'DEA',\n",
       " 'coll',\n",
       " 'ISBN',\n",
       " 'PUF',\n",
       " 'coll',\n",
       " 'coll',\n",
       " 'CNRS',\n",
       " 'ISBN',\n",
       " 'EDP',\n",
       " 'CNRS',\n",
       " 'ISBN',\n",
       " 'epistemologica',\n",
       " 'holographique',\n",
       " 'mul',\n",
       " 'interuniversitaire',\n",
       " 'uvres',\n",
       " 'einstein',\n",
       " 'caltech',\n",
       " 'edu',\n",
       " 'alberteinstein',\n",
       " 'info']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(invalid_words))\n",
    "invalid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texte entre guillemets\n",
    "exp = r\"«.+?»\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
